{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83184e7",
   "metadata": {},
   "source": [
    "# Sistemas de Ecuaciones Lineales y Números de Condición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1956d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234bab4e",
   "metadata": {},
   "source": [
    "## El problema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58e747",
   "metadata": {},
   "source": [
    "Estamos interesados en resolver sistemas de ecuaciones lineales que escribimos en la forma\n",
    "\n",
    "\\begin{equation}\n",
    "A {\\bf x} = {\\bf b},\n",
    "\\end{equation}\n",
    "\n",
    "donde $A$ es una matriz conocida, ${\\bf b}$ un vector conocido, y ${\\bf x}$ es el vector solución desconocido que estamos tratando de encontrar. Por convención, el sistema tiene tamaño $n$, es decir, la matriz tiene tamaño $n \\times n$ y los vectores son vectores columna de longitud $n$.\n",
    "\n",
    "Antes de intentar resolver el sistema lineal, recordemos que no todos los sistemas lineales pueden ser resueltos. Si la matriz es singular, lo cual es equivalente a $\\det(A) = 0$, entonces la matriz no puede ser invertida y no hay solución.\n",
    "\n",
    "Ahora consideremos la precisión con la que se pueden conocer los coeficientes de $A$ y ${\\bf b}$. Los números reales no pueden almacenarse en una computadora con precisión infinita, por lo que los coeficientes no pueden ser precisos más allá, por ejemplo, de 16 cifras significativas. En la mayoría de los casos interesantes, incluso esto es demasiado optimista: los coeficientes de $A$ y ${\\bf b}$ usualmente serán el resultado de algún otro procedimiento numérico o experimental con sus propias inexactitudes inherentes.\n",
    "\n",
    "Esto implica un punto crucial. Puede haber sistemas lineales definidos por $\\left( A, {\\bf b} \\right)$ que no queramos resolver, ya que es imposible estar seguros de que la solución sea suficientemente precisa. Es decir, un \"pequeño\" cambio en los coeficientes de, por ejemplo, $A$, puede llevar a un \"gran\" cambio en la solución. Usualmente interpretaríamos \"pequeño\" como dentro de la precisión con la que conocemos los coeficientes. Lo que significa \"gran\" depende de la precisión que requerimos en la solución, y es dependiente del problema.\n",
    "\n",
    "Si el sistema lineal tiene una solución confiable, es decir, pequeños cambios en los coeficientes conducen a pequeños cambios en la solución, lo llamamos bien condicionado. Si no es así, lo llamamos mal condicionado. Si el sistema lineal está mal condicionado, entonces no puede resolverse de manera confiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fac92f",
   "metadata": {},
   "source": [
    "Now consider the accuracy with which the coefficients of $A$ and ${\\bf b}$ can be known. Real numbers cannot be stored on a computer to infinite precision, so the coefficients cannot be accurate beyond, for example, 16 significant figures. In most interesting cases even this is too optimistic: the coefficients of $A$ and ${\\bf b}$ will usually be the result of some other numerical or experimental procedure with its own inherent inaccuracies. \n",
    "\n",
    "This implies one crucial point. There may be linear systems defined by $\\left( A, {\\bf b} \\right)$ that we do not want to solve, as it is _impossible_ to be sure that the solution is sufficiently accurate. That is, a \"small\" change in the coefficients of e.g. $A$, can lead to a \"large\" change in the solution. We would usually interpret \"small\" to mean within the accuracy with which we know the coefficients. What \"large\" means depends on the accuracy we require on the solution, and is problem dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c0aab",
   "metadata": {},
   "source": [
    "If the linear system does have a reliable solution - that is, small changes in the coefficients lead to small changes in the solution - we call it **well conditioned**. If it does not, we call it **badly conditioned**. If the linear system is badly conditioned then it cannot be reliably solved: find a different problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e7e4d",
   "metadata": {},
   "source": [
    "## Número de condición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb6b9f",
   "metadata": {},
   "source": [
    "No podemos, en la práctica, resolver no solo un sistema lineal, sino muchos problemas vecinos, solo para verificar si el sistema es razonable. En su lugar, queremos un criterio simple que podamos verificar de manera económica para ver si vale la pena resolver el sistema lineal. Podemos condensar esto en un solo número, llamado el **número de condición**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b29c4",
   "metadata": {},
   "source": [
    "### Determinantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f95c89",
   "metadata": {},
   "source": [
    "¿Por qué no usar el determinante como el número de condición? Después de todo, si el determinante es cero, la matriz no puede ser invertida. Sin embargo, considera la siguiente matriz:\n",
    "\n",
    "\\begin{equation}\n",
    "  A = 10^{-1} \\begin{pmatrix} 1 & 0 & \\dots & 0 \\\\ 0 & 1 & \\ddots & \\vdots \\\\ \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\dots & 0 & 1 \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Esta matriz diagonal se puede invertir de manera trivial y se comporta perfectamente bien, sin importar su tamaño. Sin embargo, $\\det(A) = 10^{-n}$, lo cual es arbitrariamente pequeño para un $n$ suficientemente grande. Por lo tanto, el determinante no puede ser un buen número de condición."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73599598",
   "metadata": {},
   "source": [
    "## Normas de vectores y matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2757f8",
   "metadata": {},
   "source": [
    "### Normas de vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3514d55",
   "metadata": {},
   "source": [
    "Una norma es una función de distancia matemática. Las normas estándar para, por ejemplo, vectores reales, usan los tamaños de los componentes o la \"longitud\" del vector. Las más útiles para nuestros propósitos son las normas $1, 2$ y $\\infty$:\n",
    "\n",
    "\\begin{align}\n",
    "  \\| x \\|_{1} & = \\sum_{j = 1}^n | x_j |, \\\\\n",
    "  \\| x \\|_{2} & = \\sqrt{\\sum_{j = 1}^n ( x_j )^2}, \\\\\n",
    "  \\| x \\|_{\\infty} & = \\max_{j} | x_j |.\n",
    "\\end{align}\n",
    "\n",
    "Por ejemplo, la norma $2$ es la distancia \"estándar\" pitagórica.\n",
    "\n",
    "Ten en cuenta que diferentes normas dan respuestas diferentes cuando se aplican al *mismo* vector. Por ejemplo, si ${\\bf x} = (-1, 2, 1)$ entonces\n",
    "\n",
    "\\begin{align}\n",
    "  \\| x \\|_{1} & = |-1| + |2| + |1| && = 4, \\\\\n",
    "  \\| x \\|_{2} & = \\sqrt{(-1)^2 + (2)^2 + (1)^2} && = \\sqrt{6}, \\\\\n",
    "  \\| x \\|_{\\infty} & = \\max_{j} \\left(|-1|, |2|, |1| \\right) && = 4.\n",
    "\\end{align}\n",
    "\n",
    "Por lo tanto, al comparar normas de diferentes vectores, es necesario usar siempre la misma norma.\n",
    "\n",
    "Como una ilustración de las diferentes normas, restringimos a vectores de 2 dimensiones (que son fáciles de visualizar) y trazamos todos los vectores con norma 1 para cada una de estas tres normas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf0ce69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norma L1:  6.0\n",
      "norma L2:  3.7416573867739413\n",
      "norma L_inf:  3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as la\n",
    "\n",
    "b = np.array([1,2,3])\n",
    "\n",
    "N1   = la.norm(b, 1)\n",
    "N2  = la.norm(b, 2)\n",
    "Ninf = la.norm(b, np.inf)\n",
    "\n",
    "print(\"norma L1: \", N1)\n",
    "print(\"norma L2: \", N2)\n",
    "print(\"norma L_inf: \", Ninf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7613959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la magnitud o norma L2 de b es:  3.7416573867739413\n",
      "La magnitud o norma L2 de b es:  2.6457513110645907\n"
     ]
    }
   ],
   "source": [
    "# Otras formas de calcular la norma L2\n",
    "import math as m\n",
    "\n",
    "print(\"la magnitud o norma L2 de b es: \", m.sqrt(1**2 + 2**2 + 3**2))\n",
    "print(\"La magnitud o norma L2 de b es: \", m.sqrt(np.sum(b**2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ae27d",
   "metadata": {},
   "source": [
    "### Normas de matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757dac6",
   "metadata": {},
   "source": [
    "Realmente queremos la norma de la matriz, $\\| A \\|$. Podemos usar cualquier vector ${\\bf y}$ combinado con cualquier norma de vector para *inducir* una norma de matriz definiendo\n",
    "\n",
    "\\begin{equation}\n",
    "  \\| A \\|_{\\bf y} = \\| A {\\bf y} \\|\n",
    "\\end{equation}\n",
    "\n",
    "donde la norma en el lado derecho es una norma de vector. Por supuesto, esta definición depende de la elección del vector ${\\bf y}$. Entonces podríamos maximizar sobre *todos* los vectores ${\\bf y}$: sin embargo, el tamaño de la norma depende del tamaño del vector. Por lo tanto, queremos sacar la magnitud del vector ${\\bf y}$: definimos la **norma de matriz inducida** como\n",
    "\n",
    "\\begin{equation}\n",
    "  \\| A \\| = \\max_{{\\bf y}: \\| {\\bf y} \\| = 1} \\| A {\\bf y} \\|.\n",
    "\\end{equation}\n",
    "\n",
    "En toda esta definición, todas las normas *deben* ser las mismas. Por ejemplo, la norma $2$ de la matriz se define como\n",
    "\n",
    "\\begin{equation}\n",
    "  \\| A \\|_2 = \\max_{{\\bf y}: \\| {\\bf y} \\|_2 = 1} \\| A {\\bf y} \\|_2.\n",
    "\\end{equation}\n",
    "\n",
    "Calcular manualmente la norma de la matriz usando esta definición es obviamente difícil. Sin embargo, se puede demostrar que dos normas particulares pueden simplificarse en gran medida. Es decir,\n",
    "\n",
    "1. la norma $1$ de la matriz se obtiene como el máximo de la norma $1$ de los vectores *columna* de $A$;\n",
    "2. la norma $\\infty$ de la matriz se obtiene como el máximo de la norma $1$ de los vectores *fila* de $A$.\n",
    "\n",
    "Nota que, en contraste con todo lo demás en esta sección, la norma $1$ se usa para los vectores en ambos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aaa496",
   "metadata": {},
   "source": [
    "## Número de Condición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f93c06",
   "metadata": {},
   "source": [
    "Formalmente, ahora tenemos todas las herramientas necesarias para calcular nuestro número de condición. Nuevamente, el número de condición depende de la norma utilizada en su cálculo, pero esperaríamos que todos los resultados tengan una magnitud similar.\n",
    "\n",
    "De manera general, interpretamos el número de condición como una medida de *la cantidad en que la inversión de la matriz aumentará cualquier error intrínseco en los coeficientes*.\n",
    "\n",
    "Más precisamente, podemos observar el error relativo que podemos medir: el *residual ponderado*\n",
    "\n",
    "\\begin{equation}\n",
    "  {\\cal E} = \\frac{\\| A {\\bf x}_{\\text{num}} - {\\bf b}\\|}{\\| {\\bf b} \\|}.\n",
    "\\end{equation}\n",
    "\n",
    "Aquí ${\\bf x}_{\\text{num}}$ es la solución numérica. En principio, el residual ponderado podría minimizarse, pero nunca podemos garantizar que sea precisamente cero.\n",
    "\n",
    "Ahora, se puede demostrar que\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{1}{K(A)} {\\cal E} \\le \\frac{\\| {\\bf x}_{\\text{num}} - {\\bf x}_{\\text{exact}} \\|}{\\| {\\bf x}_{\\text{exact}} \\|} \\le K(A) {\\cal E}.\n",
    "\\end{equation}\n",
    "\n",
    "El límite inferior no es importante aquí: el punto es el límite superior. Supongamos que hemos minimizado el residual ponderado a, por ejemplo, $10^{-16}$. Entonces, si el número de condición es $\\sim 10^{15}$, el mejor límite sobre el error relativo es $\\sim 0.1$: en otras palabras, ¡solo podemos garantizar la corrección del *primer* dígito significativo de nuestra solución!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5522c8",
   "metadata": {},
   "source": [
    "# Solución de Sistemas Lineales via Eliminación Gaussiana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc302260",
   "metadata": {},
   "source": [
    "Recordamos que el problema que estamos tratando de resolver es el sistema de ecuaciones lineales escrito como\n",
    "\n",
    "\\begin{equation}\n",
    "  A {\\bf x} = {\\bf b}.\n",
    "\\end{equation}\n",
    "\n",
    "Esto es equivalente a $n$ ecuaciones lineales simultáneas para las $n$ incógnitas $x_i$.\n",
    "\n",
    "La primera clase de métodos que consideramos son **directos**: se siguen un número finito de pasos que, en principio, resultan en la respuesta correcta.\n",
    "\n",
    "La mayoría de los métodos directos utilizan el mismo enfoque básico: convertir el sistema lineal en un problema equivalente que sea fácil de resolver.\n",
    "\n",
    "Entonces, lo primero que debemos hacer es considerar qué sistemas lineales son fáciles de resolver. Obviamente, si la matriz es diagonal, es fácil de resolver. Más generalmente, es sencillo resolver un sistema lineal si la matriz es *triangular*.\n",
    "\n",
    "Como ejemplo, consideremos\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{pmatrix} 1 & 2 & 3 \\\\ 0 & 4 & 5 \\\\ 0 & 0 & 6 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 9 \\\\ 6 \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Realizamos la **sustitución hacia atrás** resolviendo de abajo hacia arriba. Es decir, leemos la ecuación definida por la última fila de la matriz:\n",
    "\n",
    "\\begin{equation}\n",
    "  6 x_3 = 6 \\qquad \\implies \\qquad x_3 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Luego leemos la ecuación definida por la segunda fila de la matriz, utilizando la información encontrada para $x_3$:\n",
    "\n",
    "\\begin{equation}\n",
    "  4 x_2 + 5 x_3 = 9 \\qquad \\implies \\qquad 4 x_2 = 4 \\qquad \\implies \\qquad x_2 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Finalmente, leemos la ecuación definida por la primera fila de la matriz, utilizando la información encontrada para $x_2$ y $x_3$:\n",
    "\n",
    "\\begin{equation}\n",
    "  x_1 + 2 x_2 + 3 x_3 = 6 \\qquad \\implies \\qquad x_1 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "La sustitución hacia atrás funcionará para cualquier matriz *triangular superior*, siempre que ningún elemento diagonal sea cero (lo cual está garantizado si la matriz es no singular). Si la matriz es triangular inferior, empezaríamos desde la parte superior y trabajaríamos hacia abajo, utilizando la **sustitución hacia adelante**.\n",
    "\n",
    "## Eliminación gaussiana\n",
    "\n",
    "Entonces, necesitamos poder convertir nuestro sistema lineal original en un sistema equivalente en forma triangular; aquí apuntaremos a construir una matriz triangular superior. Por equivalente, queremos decir que la solución es la misma.\n",
    "\n",
    "Piensa en cada fila como correspondiente a una sola ecuación lineal, y cada columna como correspondiente a una variable desconocida. Entonces, vemos que hay tres operaciones básicas que se pueden realizar sin cambiar la solución:\n",
    "\n",
    "1. Multiplicar cualquier fila por un escalar no nulo (equivalente a escalar toda la ecuación).\n",
    "2. Intercambiar dos filas (equivalente a cambiar el orden de las ecuaciones).\n",
    "3. Intercambiar dos columnas (equivalente a cambiar el orden de las variables desconocidas).\n",
    "\n",
    "Nos concentraremos en las dos primeras operaciones, ya que intercambiar las columnas requiere que hagamos un seguimiento de qué entradas del vector solución final corresponden a qué variables desconocidas.\n",
    "\n",
    "Para asegurarnos de que las operaciones se realicen de manera coherente tanto en las entradas de la matriz $A$ *como* en el vector del lado derecho ${\\bf b}$, es mejor construir la **matriz aumentada** $\\left( A | {\\bf b} \\right)$. Todas las operaciones se aplican a esta matriz, que tiene $n+1$ columnas y $n$ filas.\n",
    "\n",
    "### Algoritmo básico\n",
    "\n",
    "El algoritmo básico toma la matriz aumentada con coeficientes arbitrarios y devuelve una matriz aumentada triangular superior. Luego, se puede dividir en la forma del sistema original y resolver mediante sustitución hacia atrás.\n",
    "\n",
    "El algoritmo considera cada fila a su vez. Supongamos\n",
    "\n",
    "1. Estamos observando la fila $i$, donde $1 \\le i \\le n$;\n",
    "2. Los pasos previos del algoritmo significan que los coeficientes en las primeras $i-1$ columnas de la fila $i$, y todas las filas debajo de la fila $i$ son cero.\n",
    "3. El $i^{\\text{ésimo}}$ elemento de la fila $i$, $a_{i, i}$, no es cero.\n",
    "\n",
    "Entonces podemos eliminar todos los coeficientes en la columna $i$ en las filas debajo de $i$ restando una adecuada de la fila $i$. Explícitamente, consideremos la fila $j$ donde $i < j \\le n$. Para todos los coeficientes en la fila $j$, es decir, para $a_{j, k}$ con $1 \\le k \\le n$, reemplazamos el coeficiente por\n",
    "\n",
    "\\begin{equation}\n",
    "  a_{j, k} \\rightarrow a_{j, k} - \\frac{a_{j, i}}{a_{i, i}} a_{i, k}.\n",
    "\\end{equation}\n",
    "\n",
    "Vemos que cuando $k = i$ el coeficiente $a_{j, i}$ se establecerá en cero, como se requiere.\n",
    "\n",
    "Si repetimos este algoritmo para todas las filas $i$, habremos construido una matriz triangular.\n",
    "\n",
    "Escribamos este algoritmo explícitamente en código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d3c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def GaussianEliminationReduction(A, b):\n",
    "    \"\"\"\n",
    "    Simple Eliminación Gaussiana algorithm. Solves linear system A x = b, assuming well-conditioned square matrix A.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    A : float, numpy array, 2d.\n",
    "        Square matrix, size n x n.\n",
    "    b : float, numpy array, 1d.\n",
    "        RHS vector, size n (must conform with A).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store size of system\n",
    "    n = len(b)\n",
    "    assert(np.all(A.shape == (n, n)))\n",
    "    \n",
    "    # Form augmented matrix\n",
    "    aug = np.hstack((A, b.reshape(n,1)));\n",
    "    \n",
    "    # Loop over rows\n",
    "    for i in range(n):\n",
    "        # Loop over rows below i\n",
    "        for j in range(i+1, n):\n",
    "            aug[j, :] = aug[j, :] - aug[j, i] / aug[i, i] * aug[i, :]\n",
    "    \n",
    "    # Return the separated, reduced, matrix and right hand side vector.\n",
    "    return (aug[:,:-1], aug[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9552506",
   "metadata": {},
   "source": [
    "Ahora prueba el algoritmo en algunos ejemplos simples. El primero ya está en forma escalonada o triangular, como se muestra arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc5f2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting triangular matrix is \n",
      "[[2.  1.  1. ]\n",
      " [0.  2.5 1.5]\n",
      " [0.  0.  1.6]].\n",
      " Right hand side vector becomes [8.  9.  3.6].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2.0, 1.0, 1.0], \\\n",
    "              [1.0, 3.0, 2.0], \\\n",
    "              [1.0, 2.0, 3.0]])\n",
    "b = np.array([8.0, 13.0, 13.0])\n",
    "\n",
    "(A1, b1) = GaussianEliminationReduction(A, b)\n",
    "print(\"La matriz escalonada resultante es \\n{}.\\n El vector b pasa a ser {}.\\n\".format(A1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf8844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting triangular matrix is \n",
      "[[1. 2. 3.]\n",
      " [0. 4. 5.]\n",
      " [0. 0. 6.]].\n",
      " Right hand side vector becomes [6. 9. 6.].\n",
      "\n",
      "Resulting triangular matrix is \n",
      "[[3. 0. 1.]\n",
      " [0. 2. 2.]\n",
      " [0. 0. 1.]].\n",
      " Right hand side vector becomes [4. 2. 1.].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1.0, 2.0, 3.0], \\\n",
    "              [0.0, 4.0, 5.0], \\\n",
    "              [0.0, 0.0, 6.0]])\n",
    "b = np.array([6.0, 9.0, 6.0])\n",
    "\n",
    "(A1, b1) = GaussianEliminationReduction(A, b)\n",
    "print(\"La matriz escalonada resultante es  \\n{}.\\n El vector b pasa a ser {}.\\n\".format(A1, b1))\n",
    "\n",
    "C = np.array([[3.0, 0.0, 1.0], \\\n",
    "              [6.0, 2.0, 4.0], \\\n",
    "              [9.0, 2.0, 6.0]])\n",
    "d = np.array([4.0, 10.0, 15.0])\n",
    "\n",
    "(A2, b2) = GaussianEliminationReduction(C, d)\n",
    "print(\"La matriz escalonada resultante es  \\n{}.\\n El vector b pasa a ser {}.\\n\".format(A2, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98aaf4",
   "metadata": {},
   "source": [
    "### Pivotamiento\n",
    "\n",
    "Sin embargo, el algoritmo simple anterior no funciona de manera robusta. Como un ejemplo sencillo, considera el sistema a continuación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.array([[1.0, 1.0, 1.0], \\\n",
    "              [0.0, 0.0, 2.0], \\\n",
    "              [0.0, 1.0, 1.0]])\n",
    "f = np.array([1.0, 1.0, 2.0])\n",
    "\n",
    "(A3, b3) = GaussianEliminationReduction(E, f)\n",
    "print(\"Resulting triangular matrix is \\n{}.\\n Right hand side vector becomes {}.\\n\".format(A3, b3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f6efa",
   "metadata": {},
   "source": [
    "El algoritmo ha fallado porque la matriz no cumple con las suposiciones necesarias: en particular, el elemento $a_{2, 2}$ es cero.\n",
    "\n",
    "Podemos simplemente evitar este problema intercambiando filas. En este caso, intercambiaríamos la segunda y la tercera fila y el algoritmo funcionaría (en este caso, intercambiar la segunda y la tercera fila pone la matriz en forma triangular).\n",
    "\n",
    "Sin embargo, hay un problema similar si una entrada es muy pequeña, pero no es cero. Considera el problema\n",
    "\n",
    "\\begin{equation}\n",
    "  \\left( \\begin{array}{c c|c} \\varepsilon & 1 & 1 \\\\ 1 & 1 & 2 \\end{array} \\right) \\rightarrow \\left( \\begin{array}{c c|c} \\varepsilon & 1 & 1 \\\\ 0 & 1 - \\varepsilon^{-1} & 2 - \\varepsilon^{-1} \\end{array} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "que ha sido reducido a forma triangular usando la eliminación gaussiana. Si $\\varepsilon$ es muy pequeño, entonces los términos $\\varepsilon^{-1}$ dominarán, haciendo que el problema sea aproximadamente\n",
    "\n",
    "\\begin{equation}\n",
    "  \\left( \\begin{array}{c c|c} \\varepsilon & 1 & 1 \\\\ 0 & - \\varepsilon^{-1} & - \\varepsilon^{-1} \\end{array} \\right) \\quad \\implies \\quad {\\bf x} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "La respuesta correcta *debería* ser ${\\bf x} = (1, 1)^T$. No hay nada mal con el número de condición. El problema es el paso de la eliminación gaussiana donde se divide por $\\varepsilon$, un número muy pequeño.\n",
    "\n",
    "Para evitar ambos problemas, la técnica estándar es **pivotar** o usar el **pivotamiento**. En cada etapa del algoritmo, estamos trabajando con la fila $i$ y buscando eliminar las entradas en las filas $j > i$. El primer paso es verificar si hay una fila $j > i$ con un coeficiente mayor (en magnitud) en la columna $i$. Si es así, intercambiamos las filas. Nota que no podemos considerar filas por encima de $i$ en la matriz, ya que estas pueden tener entradas no nulas en columnas $<i$.\n",
    "\n",
    "Modificamos el algoritmo para incluir el pivotamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianEliminationPivotingReduction(A, b):\n",
    "    \"\"\"\n",
    "    Simple Eliminación Gaussiana algorithm with pivoting. Solves linear system A x = b, \n",
    "    assuming well-conditioned square matrix A.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    A : float, numpy array, 2d.\n",
    "        Square matrix, size n x n.\n",
    "    b : float, numpy array, 1d.\n",
    "        RHS vector, size n (must conform with A).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store size of system\n",
    "    n = len(b)\n",
    "    assert(np.all(A.shape == (n, n)))\n",
    "    \n",
    "    # Form augmented matrix\n",
    "    aug = np.hstack((A, b.reshape(n,1)));\n",
    "    \n",
    "    # Loop over rows\n",
    "    for i in range(n):\n",
    "        # Find the row with largest magnitude, and then swap the rows.\n",
    "        max_row = np.argmax(aug[i:, i])\n",
    "        if (max_row): # Only swap rows if the maximum is not this row. \\\n",
    "                      # NOTE: the max_row is counted relative to i, so max_row = 0 => row i.\n",
    "            tmp               = np.copy(aug[i, :])\n",
    "            aug[i, :]         = np.copy(aug[i+max_row, :])\n",
    "            aug[i+max_row, :] = np.copy(tmp)\n",
    "        # Loop over rows below i\n",
    "        for j in range(i+1, n):\n",
    "            aug[j, :] = aug[j, :] - aug[j, i] / aug[i, i] * aug[i, :]\n",
    "    \n",
    "    # Return the separated, reduced, matrix and right hand side vector.\n",
    "    return (aug[:,:-1], aug[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78acc6",
   "metadata": {},
   "source": [
    "Prueba en el caso que falló anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1af859",
   "metadata": {},
   "outputs": [],
   "source": [
    "(A4, b4) = GaussianEliminationPivotingReduction(E, f)\n",
    "print(\"La matriz escalonada resultante es  \\n{}.\\n El vector b pasa a ser {}.\\n\".format(A4, b4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afcc19e",
   "metadata": {},
   "source": [
    "## Caso especial - sistemas tridiagonales\n",
    "\n",
    "Los sistemas tridiagonales aparecen con frecuencia en la solución de ecuaciones diferenciales, particularmente cuando se utilizan métodos de diferencias finitas.\n",
    "\n",
    "Consideremos el simple sistema $4 \\times 4$:\n",
    "\n",
    "\\begin{equation}\n",
    "        \\begin{pmatrix}\n",
    "          2 & 1 & 0 & 0 \\\\\n",
    "          1 & 2 & 1 & 0 \\\\\n",
    "          0 & 1 & 2 & 1 \\\\\n",
    "          0 & 0 & 1 & 2\n",
    "        \\end{pmatrix}\n",
    "        \\begin{pmatrix}\n",
    "          x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n",
    "        \\end{pmatrix}\n",
    "        =\n",
    "        \\begin{pmatrix}\n",
    "          -1 \\\\ 0 \\\\ 0 \\\\ -1\n",
    "        \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Procedemos como lo haríamos con la eliminación gaussiana: eliminamos el (único) coeficiente debajo de la diagonal primero, reescalando a medida que avanzamos:\n",
    "\n",
    "\\begin{equation}\n",
    "        \\begin{pmatrix}\n",
    "          1 & \\tfrac{1}{2} & 0 & 0 \\\\\n",
    "          0 & 1 & \\tfrac{2}{3} & 0 \\\\\n",
    "          0 & 0 & 1 & \\tfrac{3}{4} \\\\\n",
    "          0 & 0 & 0 & 1\n",
    "        \\end{pmatrix}\n",
    "        \\begin{pmatrix}\n",
    "          x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n",
    "        \\end{pmatrix}\n",
    "        =\n",
    "        \\begin{pmatrix}\n",
    "          -\\tfrac{1}{2} \\\\ \\tfrac{1}{3} \\\\ -\\tfrac{1}{4} \\\\ -\\tfrac{3}{5}\n",
    "        \\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "y luego utilizamos la sustitución hacia atrás para obtener\n",
    "\n",
    "\\begin{equation}\n",
    "        \\left\\{\n",
    "          \\begin{aligned}\n",
    "            x_4 & =-\\tfrac{3}{5} \\\\\n",
    "            x_3 & = \\tfrac{1}{5} \\\\\n",
    "            x_2 & = \\tfrac{1}{5} \\\\\n",
    "            x_1 & =-\\tfrac{3}{5}\n",
    "          \\end{aligned} \\right. .\n",
    "\\end{equation}\n",
    "\n",
    "La formulación general de esto es el **algoritmo de Thomas**. Tiene la ventaja significativa de que solo las tres diagonales no triviales necesitan ser almacenadas y trabajadas. Esto puede reducir significativamente la memoria requerida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "455d2e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5\n",
      "0.5 2 0\n",
      "0.0\n",
      "0.5 2 0\n",
      "0.0\n",
      "0.5 2 -1\n",
      "b:  [2 1 1 1]\n",
      "d:  [-1  0  0 -1]\n",
      "Solution vector x:\n",
      "[ 0 -1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def thomas_algorithm(a, b, c, d):\n",
    "    \"\"\"\n",
    "    Thomas algorithm for solving tridiagonal systems of equations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : numpy.ndarray\n",
    "        Lower diagonal of the tridiagonal matrix, size n-1.\n",
    "    b : numpy.ndarray\n",
    "        Main diagonal of the tridiagonal matrix, size n.\n",
    "    c : numpy.ndarray\n",
    "        Upper diagonal of the tridiagonal matrix, size n-1.\n",
    "    d : numpy.ndarray\n",
    "        Right-hand side vector, size n.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : numpy.ndarray\n",
    "        Solution vector, size n.\n",
    "    \"\"\"\n",
    "    n = len(b)\n",
    "    x = np.zeros_like(b)\n",
    "    bp = np.zeros_like(b)\n",
    "    dp = np.zeros_like(d)\n",
    "    \n",
    "    # Forward elimination\n",
    "    for i in range(1, n):\n",
    "        m = a[i-1] / b[i-1]\n",
    "        bp[i] = b[i] - (m * c[i-1])\n",
    "        dp[i] = d[i] - (m * d[i-1])\n",
    "        print(m * d[i-1])\n",
    "        print(m, b[i], d[i])\n",
    "    \n",
    "    # Back substitution\n",
    "    x[n-1] = dp[n-1] / bp[n-1]\n",
    "    dp[0] = d[0]\n",
    "    bp[0] = b[0]\n",
    "    for i in range(n-2, -1, -1):\n",
    "        x[i] = (dp[i] - c[i] * x[i+1]) / bp[i]\n",
    "    \n",
    "    print(\"b: \", bp)\n",
    "    print(\"d: \", dp)\n",
    "    return x\n",
    "\n",
    "# Example using the thomas_algorithm function\n",
    "a = np.array([1, 1, 1])  # lower diagonal\n",
    "b = np.array([2, 2, 2, 2])  # main diagonal\n",
    "c = np.array([1, 1, 1])  # upper diagonal\n",
    "d = np.array([-1, 0, 0, -1])  # right-hand side vector\n",
    "\n",
    "x = thomas_algorithm(a, b, c, d)\n",
    "print(\"Solution vector x:\")\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc370507",
   "metadata": {},
   "source": [
    "# Métodos de descomposición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9821d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d829c8",
   "metadata": {},
   "source": [
    "En la eliminación gaussiana realizamos operaciones de fila (o columna) para reducir el sistema lineal\n",
    "\n",
    "\\begin{equation}\n",
    "  A {\\bf x} = {\\bf b}\n",
    "\\end{equation}\n",
    "\n",
    "a una forma que fuera fácil de resolver, esencialmente transformando a un sistema equivalente donde la matriz es triangular.\n",
    "\n",
    "Por ejemplo, supongamos que podemos escribir\n",
    "\n",
    "\\begin{equation}\n",
    "  A = L U\n",
    "\\end{equation}\n",
    "\n",
    "donde $L$ es una matriz triangular inferior y $U$ es triangular superior. En este caso, el sistema lineal original es equivalente a los dos sistemas\n",
    "\n",
    "\\begin{equation}\n",
    "  A {\\bf x} = {\\bf b} \\qquad \\Leftrightarrow \\qquad \\left\\{ \\begin{aligned} L {\\bf y} & = {\\bf b}, \\\\ U {\\bf x} & = {\\bf y}. \\end{aligned} \\right. \n",
    "\\end{equation}\n",
    "\n",
    "Como tanto $L$ como $U$ son triangulares, los dos sistemas que definen se pueden resolver fácilmente utilizando sustitución hacia adelante y hacia atrás, respectivamente. Por lo tanto, *si* podemos descomponer la matriz $A$ como el producto de matrices triangulares, el sistema lineal se resuelve fácilmente.\n",
    "\n",
    "## Ejemplo\n",
    "\n",
    "Consideremos el sistema\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{pmatrix} 2 & 1 & -1 \\\\ 4 & 1 & 0 \\\\ -2 & -3 & 8 \\end{pmatrix} {\\bf x} = \\begin{pmatrix} 0 \\\\ 6 \\\\ -12 \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Veremos más adelante que la matriz se puede descomponer como\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{pmatrix} 2 & 1 & -1 \\\\ 4 & 1 & 0 \\\\ -2 & -3 & 8 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ -1 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 1 & -1 \\\\ 0 & -1 & 2 \\\\ 0 & 0 & 3 \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Podemos verificar rápidamente que."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2.0, 1.0, -1.0] ,[4.0,  1.0, 0.0], [-2.0, -3.0, 8.0]])\n",
    "L = np.array([[1.0, 0.0,  0.0] ,[2.0,  1.0, 0.0], [-1.0,  2.0, 1.0]])\n",
    "U = np.array([[2.0, 1.0, -1.0] ,[0.0, -1.0, 2.0], [ 0.0,  0.0, 3.0]])\n",
    "np.dot(L, U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a129ef8",
   "metadata": {},
   "source": [
    "Luego resolvemos $L {\\bf y} = {\\bf b}$ mediante sustitución hacia adelante:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{pmatrix}\n",
    "    1 & 0 & 0 \\\\\n",
    "    2 & 1 & 0 \\\\\n",
    "   -1 & 2 & 1\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    y_1 \\\\ y_2 \\\\ y_3\n",
    "  \\end{pmatrix} =\n",
    "  \\begin{pmatrix}\n",
    "    0 \\\\ 6 \\\\ -12\n",
    "  \\end{pmatrix}\n",
    "  \\Rightarrow \\left\\{\n",
    "  \\begin{aligned}\n",
    "    y_1 & = 0, \\\\\n",
    "    y_2 & = 6, \\\\\n",
    "    y_3 & = -24.\n",
    "  \\end{aligned}\n",
    "  \\right.\n",
    "\\end{equation}\n",
    "\n",
    "Finalmente resolvemos $U {\\bf x} = {\\bf y}$ mediante sustitución hacia atrás:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{pmatrix}\n",
    "    2 & 1 & -1 \\\\\n",
    "    0 & -1 & 2 \\\\\n",
    "    0 & 0 & 3\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    x_1 \\\\ x_2 \\\\ x_3\n",
    "  \\end{pmatrix} =\n",
    "  \\begin{pmatrix}\n",
    "    0 \\\\ 6 \\\\ -24\n",
    "  \\end{pmatrix}\n",
    "  \\Rightarrow \\left\\{\n",
    "  \\begin{aligned}\n",
    "    x_1 & = 7, \\\\\n",
    "    x_2 & = -22, \\\\\n",
    "    x_3 & = -8.\n",
    "  \\end{aligned}\n",
    "  \\right.\n",
    "\\end{equation}\n",
    "\n",
    "Nuevamente podemos verificar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([0.0, 6.0, -12.0])\n",
    "la.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b060f",
   "metadata": {},
   "source": [
    "## Ejemplo de factorización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27c367",
   "metadata": {},
   "source": [
    "El ejemplo anterior tenía la matriz\n",
    "\n",
    "\\begin{equation}\n",
    "    A =\n",
    "    \\begin{pmatrix}\n",
    "      2 & 1 & -1 \\\\\n",
    "      4 & 1 & 0 \\\\\n",
    "      -2 & -3 & 8\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "que descomponemos usando\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{i j} = \\sum_{s=1}^{\\min(i, j)} \\ell_{i s} u_{s j}.\n",
    "\\end{equation}\n",
    "\n",
    "El primer coeficiente, $i = j = 1$, cumple\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{1 1}  = 2 = \\ell_{1 1} u_{1 1}.\n",
    "\\end{equation}\n",
    "\n",
    "Usamos nuestra libertad para fijar algunos coeficientes y elegimos $\\ell_{1 1} = 1$, por ejemplo, lo que da $u_{1 1} = a_{1 1} = 2$.\n",
    "\n",
    "Ahora consideremos la primera fila de $U$ ($i = 1$, $j$ libre) y la primera columna de $L$ ($i$ libre, $j = 1$). La primera fila de $U$ cumple\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{1 j} = \\ell_{1 1} u_{1 j} = u_{1 j}\n",
    "\\end{equation}\n",
    "\n",
    "y la primera columna de $L$ cumple\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{i 1} = \\ell_{i 1} u_{1 1} = 2 \\ell_{i 1}.\n",
    "\\end{equation}\n",
    "\n",
    "Por lo tanto, sabemos que\n",
    "\n",
    "\\begin{equation}\n",
    "    L =\n",
    "    \\begin{pmatrix}\n",
    "      1 & 0 & 0 \\\\\n",
    "      \\tfrac{4}{2} & ?? & 0 \\\\\n",
    "      \\tfrac{-2}{2} & ?? & ??\n",
    "    \\end{pmatrix}, \\quad\n",
    "    U =\n",
    "    \\begin{pmatrix}\n",
    "      2 & 1 & -1 \\\\\n",
    "      0 & ?? & ?? \\\\\n",
    "      0 & 0 & ??\n",
    "    \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Pasamos a la segunda fila y columna.\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{2 2} = \\ell_{2 1} u_{1 2} + \\ell_{2 2} u_{2 2}.\n",
    "\\end{equation}\n",
    "\n",
    "Ya hemos calculado las entradas $\\ell_{2 1} = 2$, $u_{1 2} = 1$. Nuevamente, fijamos $\\ell_{2 2} = 1$, lo que da\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{2 2} = 1 = 2 + u_{2 2} \\Rightarrow u_{2 2} = -1.\n",
    "\\end{equation}\n",
    "\n",
    "Como en el paso anterior, consideramos la segunda fila de $U$ y la segunda columna de $L$, encontrando\n",
    "\n",
    "\\begin{align}\n",
    "    u_{2 j}    & = a_{2 j} - \\ell_{2 1} u_{1 j}, \\\\\n",
    "    \\ell_{i 2} & = \\left(a_{i 2} - \\ell_{i 1} u_{1 2} \\right) / u_{2 2},\n",
    "\\end{align}\n",
    "\n",
    "lo que implica que\n",
    "\n",
    "\\begin{align}\n",
    "    u_{2 3}    & = 0 - 2 \\times (-1) = 2, \\\\\n",
    "    \\ell_{3 2} & = \\left(-3 - (-1) \\times 1\\right) / (-1) = 2.\n",
    "\\end{align}\n",
    "\n",
    "Ahora tenemos las dos primeras filas y columnas:\n",
    "\n",
    "\\begin{equation}\n",
    "    L =\n",
    "    \\begin{pmatrix}\n",
    "      1 & 0 & 0 \\\\\n",
    "      2 & 1 & 0 \\\\\n",
    "      -1 & 2 & ??\n",
    "    \\end{pmatrix}, \\quad\n",
    "    U =\n",
    "    \\begin{pmatrix}\n",
    "      2 & 1 & -1 \\\\\n",
    "      0 & -1 & 2 \\\\\n",
    "      0 & 0 & ??\n",
    "    \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Continuando como antes, usamos la última elección libre para fijar $\\ell_{3 3} = 1$. Finalmente, como antes, calculamos que $u_{3 3} = 3$. Así que finalmente tenemos\n",
    "\n",
    "\\begin{equation}\n",
    "    L =\n",
    "    \\begin{pmatrix}\n",
    "      1 & 0 & 0 \\\\\n",
    "      2 & 1 & 0 \\\\\n",
    "      -1 & 2 & 1\n",
    "    \\end{pmatrix}, \\quad\n",
    "    U =\n",
    "    \\begin{pmatrix}\n",
    "      2 & 1 & -1 \\\\\n",
    "      0 & -1 & 2 \\\\\n",
    "      0 & 0 & 3\n",
    "    \\end{pmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d1e2d",
   "metadata": {},
   "source": [
    "## Algoritmo de Descomposición $LU$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed1d9f",
   "metadata": {},
   "source": [
    "* Escribe la fórmula explícita para los coeficientes de la matriz.\n",
    "\n",
    "  1. Trabaja desde la primera fila/columna hasta la última.\n",
    "  2. Observa la entrada diagonal de $A$: utiliza la libertad para elegir el valor de la entrada diagonal de $L$ o $U$.\n",
    "  3. La fila correspondiente de $U$ y la columna de $L$ se derivan de la fórmula explícita ya que todas las demás entradas son conocidas.\n",
    "\n",
    "* Si $u_{k k}$ o $\\ell_{k k}$ son cero, el algoritmo simple falla; sin embargo, una descomposición $LU$ puede seguir existiendo.\n",
    "* $U$ es la matriz que se encontraría usando la eliminación gaussiana. La descomposición $LU$ tiene la ventaja de que es válida para cualquier ${\\bf b}$ en $A {\\bf x} = {\\bf b}$, mientras que con la eliminación gaussiana se necesita repetir todo el algoritmo.\n",
    "* Al igual que con la eliminación gaussiana, el pivotamiento para mayor precisión es necesario para matrices generales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675d24e",
   "metadata": {},
   "source": [
    "## Codigo de ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8779e",
   "metadata": {},
   "source": [
    "Este es un ejemplo de algoritmo de descomposición $LU$. Ten en cuenta que, como no utiliza pivotamiento, es posible que falle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LU_decomposition(A):\n",
    "    \"\"\"Perform LU decomposition using the Doolittle factorisation.\"\"\"\n",
    "    \n",
    "    L = np.zeros_like(A)\n",
    "    U = np.zeros_like(A)\n",
    "    N = np.size(A, 0)\n",
    "    \n",
    "    for k in range(N):\n",
    "        L[k, k] = 1\n",
    "        U[k, k] = (A[k, k] - np.dot(L[k, :k], U[:k, k])) / L[k, k]\n",
    "        for j in range(k+1, N):\n",
    "            U[k, j] = (A[k, j] - np.dot(L[k, :k], U[:k, j])) / L[k, k]\n",
    "        for i in range(k+1, N):\n",
    "            L[i, k] = (A[i, k] - np.dot(L[i, :k], U[:k, k])) / U[k, k]\n",
    "    \n",
    "    return L, U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0708d90",
   "metadata": {},
   "source": [
    "Probemos este código con el ejemplo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9097224",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2, U2 = LU_decomposition(A)\n",
    "print(\"Compare the code and hand answers for L:\\n{},\\n{}\\nand for U:\\n{},\\n{}\".format(L2, L, U2, U))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
